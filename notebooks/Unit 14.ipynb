{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional uses for `for`-loops\n",
    "\n",
    "This unit directly continues the previous one.\n",
    "We now have a function to calculate the average number of tokens per word type in a text.\n",
    "This allows us to estimate how often an author reuses words in the text.\n",
    "But it would still be nice to get a few other metrics, such as\n",
    "\n",
    "1. the frequencies of word types rather than their total counts (this makes it easier to compare different texts since 50 mentions of \"buletic\" in a 1000-page novel doesn't have the same weight as 50 mentions in a 1000-word essay),\n",
    "1. the average word length.\n",
    "\n",
    "Before we continue, though, we once again have to run all the relevant code to get counts for our three texts *Hamlet*, *Dr. Faustus*, and *Princess of Mars*.\n",
    "This is exactly the same code as what we had at the beginning of the previous unit.\n",
    "Make sure you run the cell every time you use this notebook, otherwise many functions will be undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# we first define custom functions for all individual steps\n",
    "\n",
    "def get_file(text):\n",
    "    if text == \"hamlet\":\n",
    "        urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/1524/pg1524.html\", \"hamlet.txt\")\n",
    "    if text == \"faustus\":\n",
    "        urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/811/pg811.txt\", \"faustus.txt\")\n",
    "    if text == \"johncarter\":\n",
    "        urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/62/pg62.txt\", \"johncarter.txt\")\n",
    "        \n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as text:\n",
    "        return text.read()\n",
    "    \n",
    "def delete_before_line(string, line):\n",
    "    return str.split(string, \"\\n\", line)[-1]\n",
    "\n",
    "def delete_after_line(string, line):\n",
    "    return str.join(\"\\n\", str.split(string, \"\\n\")[:line+1])\n",
    "\n",
    "def hamlet_cleaner(text):\n",
    "    # 0. delete unwanted lines\n",
    "    text = delete_after_line(delete_before_line(text, 366), 10928)\n",
    "    # 1. remove all headers, i.e. lines starting with <h1, <h2, <h3, and so on\n",
    "    text = re.sub(r\"<h[0-9].*\", r\"\", text)\n",
    "    # 2. remove speaker information, i.e. lines of the form <p id=\"id012345789\"...<br/>\n",
    "    text = re.sub(r'<p id=\"id[0-9]*\">[^<]*<br/>', r\"\", text)\n",
    "    # 3. remove html tags, i.e. anything of the form <...>\n",
    "    text = re.sub(r\"<[^>]*>\", r\"\", text)\n",
    "    # 4. remove anything after [ or before ] on a line (this takes care of stage descriptions)\n",
    "    text = re.sub(r\"\\[[^\\]\\n]*\", r\"\", text)\n",
    "    text = re.sub(r\"[^\\[\\n]*\\]\", r\"\", text)\n",
    "    return text\n",
    "\n",
    "def faustus_cleaner(text):\n",
    "    # 0. delete unwanted lines\n",
    "    text = delete_after_line(delete_before_line(text, 139), 2854)\n",
    "    # 1. remove stage information\n",
    "    #    (anything after 10 spaces)\n",
    "    text = re.sub(r\"(\\s){10}[^\\n]*\", r\"\", text)\n",
    "    # 2. remove speaker information\n",
    "    #    (any word in upper caps followed by space or dot)\n",
    "    text = re.sub(r\"[A-Z]{2,}[\\s\\.]\", r\"\", text)\n",
    "    # 3. remove anything between square brackets (this takes care of footnote markers)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", r\"\", text)\n",
    "    return text\n",
    "\n",
    "def johncarter_cleaner(text):\n",
    "    # 0. delete unwanted lines\n",
    "    text = delete_after_line(delete_before_line(text, 234), 6940)\n",
    "    # 1. delete CHAPTER I\n",
    "    # (must be done like this because Roman 1 looks like English I)\n",
    "    text = re.sub(\"CHAPTER I\", \"\", text)\n",
    "    # 2. remove any word in upper caps that is longer than 1 character\n",
    "    text = re.sub(r\"[A-Z]{2,}\", r\"\", text)\n",
    "    # 3. remove anything after [ or before ] on a line\n",
    "    text = re.sub(r\"\\[[^\\]\\n]*\", r\"\", text)\n",
    "    text = re.sub(r\"[^\\[\\n]*\\]\", r\"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(string):\n",
    "    return re.findall(r\"\\w+\", string)\n",
    "\n",
    "def count(token_list):\n",
    "    return Counter(token_list)\n",
    "\n",
    "\n",
    "# and now we have two functions that use all the previous functions\n",
    "# to do all the necessary work for us\n",
    "def get_and_clean(text):\n",
    "    get_file(text)\n",
    "    string = read_file(text + \".txt\")\n",
    "    string = str.lower(string)\n",
    "    # file-specific cleaning steps\n",
    "    if text == \"hamlet\":\n",
    "        return hamlet_cleaner(string)\n",
    "    if text == \"faustus\":\n",
    "        return faustus_cleaner(string)\n",
    "    if text == \"johncarter\":\n",
    "        return johncarter_cleaner(string)\n",
    "\n",
    "def tokenize_and_count(string):\n",
    "    return (count(tokenize(string)))\n",
    "\n",
    "# and finally we get to run all the code\n",
    "hamlet = tokenize_and_count(get_and_clean(\"hamlet\"))\n",
    "faustus = tokenize_and_count(get_and_clean(\"faustus\"))\n",
    "johncarter = tokenize_and_count(get_and_clean(\"johncarter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating frequencies\n",
    "\n",
    "The frequency of a word indicates how many percent of a text are taken up by its tokens.\n",
    "For example, if a word type has 6 tokens in a text of 1000 words, then its frequency is $\\frac{6}{1000} = 0.006 = 0.6\\%$.\n",
    "So we get the frequency of a word *w* by dividing the count of *w* by the total number of tokens in the text.\n",
    "\n",
    "We already have many of the tools that are needed to calculate frequencies.\n",
    "The code provides us with counters for the three texts, and in the previous unit we saw how to calculate average token length, which taught us a few core techniques: `for`-loops, the `len`-function, and how to use `[x]` to get the value of a specific item `x` in a counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_token_count(word_counter):\n",
    "    # keep track of total number of tokens\n",
    "    total = 0\n",
    "    # start a for-loop over the counter\n",
    "    for current_word in word_counter:\n",
    "        # add count of current word to total\n",
    "        total = total + word_counter[current_word]\n",
    "    # divide `total` by number of word types (Python uses the slash / for division)\n",
    "    average = total / len(word_counter)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recombine these techniques to define a custom function for computing word frequencies.\n",
    "\n",
    "First, we will need to determine the total number of tokens in the text.\n",
    "But the code above already tells us how to do that, that's what the variable `total` keeps track of.\n",
    "Once we know the total, we can calculate the frequency of a word type by dividing its number of tokens by `total`.\n",
    "Let us put the relevant code for these steps into a function that prints the frequency of every word type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def frequencies(word_counter):\n",
    "    # keep track of total number of tokens\n",
    "    total = 0\n",
    "    # start a for-loop over the counter\n",
    "    for current_word in word_counter:\n",
    "        # add count of current word to total\n",
    "        total = total + word_counter[current_word]\n",
    "    # we have computed the total,\n",
    "    # now we calculate frequencies for all the words in the counter\n",
    "    for current_word in word_counter:\n",
    "        number_of_tokens = word_counter[current_word]\n",
    "        frequency = number_of_tokens / total\n",
    "        print(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "The functions `avg_token_count` and `frequencies` use exactly the same code to calculate the total number of tokens.\n",
    "Convert this code into a custom function `count_total`, then change `avg_token_count` and `frequencies` so that they use `count_total` for calculating the value of `total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define count_total here, and include modified versions of avg_token_count and frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we need two loops\n",
    "\n",
    "Note that in the code below we have two `for`-loops that iterate over the same counter.\n",
    "This is because we first have to look at all elements of the counter to compute the value of `total`.\n",
    "Once we know `total`, we can again look at each element to calculate its frequency.\n",
    "We really need two distinct `for`-loops for this, it cannot be done in a single loop.\n",
    "If you don't understand why, consider the toy example below where we use only one `for`-loop and get the wrong frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define our test counter\n",
    "test = Counter([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\"])\n",
    "\n",
    "# keep track of total number of tokens\n",
    "total = 0\n",
    "# start a for-loop over the counter\n",
    "for current_word in test:\n",
    "    # add count of current word to total\n",
    "    total = total + test[current_word]\n",
    "    number_of_tokens = test[current_word]\n",
    "    frequency = number_of_tokens / total\n",
    "    # and print frequency\n",
    "    print(\"Frequency of\", current_word, \"is\", number_of_tokens, \"/\", total, \"=\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "There is actually a way to make things work with just one loop.\n",
    "The first loop is only needed to determine the total number of word tokens in the text.\n",
    "But this is the same as the length of `tokenize(text)`.\n",
    "So we could instead design our frequency function as follows:\n",
    "\n",
    "1. `frequencies` takes some text as its only argument.\n",
    "1. The function then tokenizes the text.\n",
    "1. It then determines the length of the tokenized text and stores it in the variable `total`.\n",
    "1. The rest of the function proceeds as before.\n",
    "\n",
    "Copy-paste the definition of `frequencies` into the cell below, then modify it in the fashion just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy-paste frequencies here, then modify it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding frequencies to the counter\n",
    "\n",
    "Alright, so now we know that two `for`-loops are indeed needed, but the function is still somewhat unsatisfying in that it prints the frequency of each word type.\n",
    "Printing to screen isn't very useful most of the time.\n",
    "It would be better if we could simply replace the absolute values in the counter by frequencies.\n",
    "This is actually fairly easy.\n",
    "The `[x]` notation is not only useful for retrieving the value of an element, it also allows us to **specify** the value of an element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define our test counter\n",
    "test = Counter([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\"])\n",
    "print(test)\n",
    "\n",
    "# let's change the value of \"a\";\n",
    "# here's what it is right now\n",
    "print(\"a's current value:\", test[\"a\"])\n",
    "# and now we'll change it to 0.1\n",
    "test[\"a\"] = 0.1\n",
    "print(\"a's new value:\", test[\"a\"])\n",
    "\n",
    "# and now we add a new element \"d\" to the counter\n",
    "test[\"d\"] = 10\n",
    "print(\"d was added with count:\", test[\"d\"])\n",
    "print(\"The new counter is:\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Now we can finalize the `frequencies` function.\n",
    "Instead of printing the frequency of `current_word`, the function should override the value of `current_word` in `word_counter` with `frequency`.\n",
    "At the end, the function returns `word_counter`.\n",
    "You can test your code in the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change and complete the code below\n",
    "\n",
    "def frequencies(word_counter):\n",
    "    total = 0\n",
    "    for current_word in word_counter:\n",
    "        total = total + word_counter[current_word]\n",
    "    for current_word in word_counter:\n",
    "        number_of_tokens = word_counter[current_word]\n",
    "        frequency = number_of_tokens / total\n",
    "        print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your code here\n",
    "test_counts = Counter([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\"])\n",
    "print(test_counts)\n",
    "\n",
    "test_frequency = frequencies(test_counts)\n",
    "print(test_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Unintended Side-Effect\n",
    "\n",
    "Let us run the test one more time, with just a minor change in the order of the `print`-statements.\n",
    "Now we first compute `test_frequency` and the print `test_counts` and `test_frequency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your code here\n",
    "test_counts = Counter([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\"])\n",
    "test_frequency = frequencies(test_counts)\n",
    "\n",
    "print(test_counts)\n",
    "print(test_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uhm, what's going on here?\n",
    "Why do `test_counts` and `test_frequency` look the same?\n",
    "Where did the absolute word counts go?\n",
    "\n",
    "The problem is with how we wrote the function `frequency`.\n",
    "This is a function that takes a word counter as an argument and then **overwrites** the count of each word type with its frequency.\n",
    "So if we run `frequency` over `test_counts`, all the values of `test_counts` are replaced by frequencies.\n",
    "That's not really what we want.\n",
    "Instead, we want to produce a copy of `test_counts` with frequencies while keeping the original version of `test_counts` untouched.\n",
    "We can create a copy of a counter with the function `Counter.copy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your code here\n",
    "test_counts = Counter([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\"])\n",
    "test_frequency = frequencies(Counter.copy(test_counts))\n",
    "\n",
    "print(test_counts)\n",
    "print(test_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we now longer run `frequencies` on `test_counts`, but a dynamically created copy of `test_counts`.\n",
    "Hence the values of `test_counts` remain unaltered, and we get different outputs for `print` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Copy-paste your definition of the `frequencies` function into the cell below, then change it so that it always creates a copy `temp_copy` of `word_counter` at the beginning and then carries out all operations over `temp_copy` instead of `word_counter`.\n",
    "Then run the code in the next cell to verify that your new definition of `frequencies` works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy-paste your code for frequencies here, then modify it as described"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your code here\n",
    "test_counts = Counter([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\"])\n",
    "test_frequency = frequencies(test_counts)\n",
    "\n",
    "print(test_counts)\n",
    "print(test_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating average word length\n",
    "\n",
    "Let us turn to average word length next.\n",
    "Rather than explain at length how it works, I already created a working solution for you.\n",
    "Your job is just to figure out what it does.\n",
    "I will tell you that much, though: `*` is multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_word_length(word_counter):\n",
    "    total_number = 0\n",
    "    total_length = 0\n",
    "    for word in word_counter:\n",
    "        total_number = total_number + word_counter[word]\n",
    "        total_length = total_length + (word_counter[word] * len(word))\n",
    "    return total_length / total_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Carefully read the code above.\n",
    "Then describe what this function does.\n",
    "Pay particular attention to the `for`-loop and the use of `len`, and explain why `len(word)` must be multiplied with `word_counter[word]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*put your explanation here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what happens when we run this function on our three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in [hamlet, faustus, johncarter]:\n",
    "    print(avg_word_length(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's surprisingly close together.\n",
    "One would expect a novel like *Princess of Mars* to have longer words than a play like *Hamlet* or *Dr. Faustus*, simply because the latter have to fit a specific meter.\n",
    "\n",
    "But wait a second!\n",
    "*Zipf's law* tells us that the majority of texts is made up of a few very high-frequency words, called *stop words*.\n",
    "So if those stop words are very short, then that will drastically lower average word length.\n",
    "And that is indeed the case, as you can probably tell from eyeballing [this list of stopwords](https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt).\n",
    "We can even use Python to see this point more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download list of stop words\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt\", \"stopwords.txt\")\n",
    "\n",
    "# read it in as a string\n",
    "stopwords = read_file(\"stopwords.txt\")\n",
    "\n",
    "# since each word is on its own line,\n",
    "# we can convert the string to a list of words by\n",
    "# matching everything except newlines (\\n)\n",
    "stopwords = re.findall(r\"[^\\n]+\", stopwords)\n",
    "\n",
    "# and here is what we have\n",
    "print(stopwords)\n",
    "\n",
    "# and this is the average word length\n",
    "print(\"\\nAverage word length:\", avg_word_length(Counter(stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the average word length for all three texts is very close to the average word length of English stop words.\n",
    "So there is a good chance that we will get a much more pronounced difference in average word length between the three texts if we ignore stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "\n",
    "Our goal at this point is clear: We have to get rid of those pesky stopwords.\n",
    "This is actually fairly easy with a `for`-loop.\n",
    "\n",
    "First, we need to get a tokenized list for each text.\n",
    "With the functions from the beginning of the unit, that is easy-peasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamlet_full = tokenize(get_and_clean(\"hamlet\"))\n",
    "faustus_full = tokenize(get_and_clean(\"faustus\"))\n",
    "johncarter_full = tokenize(get_and_clean(\"johncarter\"))\n",
    "\n",
    "# check the output to see what these lists look like\n",
    "for text in [hamlet_full, faustus_full, johncarter_full]:\n",
    "    print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to construct a version of these lists where all stopwords are omitted.\n",
    "We can do this as follows:\n",
    "\n",
    "1. We create an empty list `words`.\n",
    "1. We iterate over the tokenized text, and every token that is not a stop word gets added to `words`.\n",
    "1. When the `for`-loop finishes, `words` will contain all tokens of the text except the stop words.\n",
    "\n",
    "Here's what this looks like for `hamlet_full`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define hamlet_full\n",
    "hamlet_full = tokenize(get_and_clean(\"hamlet\"))\n",
    "# define list of stop words\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt\", \"stopwords.txt\")\n",
    "stopwords = re.findall(r\"[^\\n]+\", read_file(\"stopwords.txt\"))\n",
    "\n",
    "# empty list of words\n",
    "words = []\n",
    "\n",
    "# start for-loop\n",
    "for token in hamlet_full:\n",
    "    if token not in stopwords:\n",
    "        # add token to words\n",
    "        list.append(words, token)\n",
    "        \n",
    "        \n",
    "# let's compare the two by looking at the first 50 tokens\n",
    "print(hamlet_full[:50])\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "You might have noticed a minor problem: `tokenize` treats strings like *who's* as two tokens *who* and *s*.\n",
    "But our list of stop words instead treats *who* and *who's* as stop words, but not *s* because it assumes that *who's* is never split into *who* and *s*.\n",
    "As a result, the code above removes *who* from `hamlet_full`, but not *s*.\n",
    "\n",
    "Copy-paste the code into the cell below, then fix it so that *s* is also filtered out correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy-paste the code here, then modify it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Based on the code above, design a custom function `filter_words` such that:\n",
    "\n",
    "1. `filter_words` takes two lists as arguments, `token_list` and `stopwords`, and\n",
    "1. `filter_words` returns a version of `token_list` that only contains those elements that aren't also listed in `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define hamlet_full\n",
    "hamlet_full = tokenize(get_and_clean(\"hamlet\"))\n",
    "faustus_full = tokenize(get_and_clean(\"faustus\"))\n",
    "johncarter_full = tokenize(get_and_clean(\"johncarter\"))\n",
    "\n",
    "# define list of stop words\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt\", \"stopwords.txt\")\n",
    "stopwords = re.findall(r\"[^\\n]+\", read_file(\"stopwords.txt\"))\n",
    "# fixme: add s to stopwords\n",
    "\n",
    "def filter_words(tokens, stopwords):\n",
    "    words = []\n",
    "    # some mystery happens here\n",
    "    return words\n",
    "\n",
    "hamlet_filtered = filter_words(hamlet_full, stopwords)\n",
    "faustus_filtered = filter_words(faustus_full, stopwords)\n",
    "johncarter_filtered = filter_words(johncarter_full, stopwords)\n",
    "\n",
    "for text in [hamlet_filtered, faustus_filtered, johncarter_filtered]:\n",
    "    print(avg_word_length(Counter(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Has the removal of stop words led to a greater difference in word length between the texts?\n",
    "Do we see a more pronounced difference between plays and novels now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "This unit hasn't introduced a lot of new concepts, instead you just got to see how `for`-loops can be used to accomplish a variety of things.\n",
    "They really are one of Python's most important and versatile tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises.**\n",
    "We can even use `for`-loops to implement a very simple spellchecker.\n",
    "[This textfile](https://raw.githubusercontent.com/dwyl/english-words/master/words.txt) lists almost all words of English (it has over 500,000 entries).\n",
    "It is similar to our list of stopwords in that it contains one word per line.\n",
    "\n",
    "The code below downloads the file for you and reads it in as a string.\n",
    "Here is your task:\n",
    "\n",
    "1.  Tokenize the string to get a list of correctly spelled English words.\n",
    "1.  Write a function `spellcheck` such that\n",
    "    1. The function takes as its argument a string and a list of words, the dictionary.\n",
    "    1. The function tokenizes the string (words like *who's* should be tokenized as *who's*, so you'll have to modify our standard tokenization function).\n",
    "    1. It then checks every token in the string and returns a list of all misspelled tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download the file\n",
    "url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\"\n",
    "urllib.request.urlretrieve(url, \"words.txt\")\n",
    "dict_string = read_file(\"words.txt\")\n",
    "\n",
    "# Step 1: tokenize dict_string\n",
    "# dictionary = ...\n",
    "\n",
    "# Step 2: define custom function\n",
    "# def spellcheck(string, wordlist):\n",
    "    # some mystery happens here\n",
    "\n",
    "# let's test the code\n",
    "examples = [\"My neighbour is to tried for work.\",\n",
    "            \"I am teh world's gr8test typist\",\n",
    "            \"U don't look 2 bed\",\n",
    "            \"Their are titilating oportunitys here.\"]\n",
    "for example in examples:\n",
    "    print(example, \"contains the following errors:\", spellcheck(example, dictionary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
