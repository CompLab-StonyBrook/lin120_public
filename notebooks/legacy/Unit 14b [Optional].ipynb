{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion Unit: Sets\n",
    "\n",
    "By now we know several types of objects in Python:\n",
    "\n",
    "1. strings (`\"This is a string\"`),\n",
    "1. integers (`5`, `18300`, `-523`),\n",
    "1. floats (`5.0`, `18300.3`, `-523.21223519`),\n",
    "1. Booleans (`True`, `False`),\n",
    "1. lists (`[\"a\", \"b\", \"c\"]`, `[]`, `[10, 10, 5, 8]`)\n",
    "1. Counters (`{\"a\": 6, \"b\": 4, \"c\": 2}`)\n",
    "\n",
    "Python provides many other data types, each one of which serves a specific purpose.\n",
    "The data types above cover almost all general usage cases, but sometimes a specific data structure is more convenient or more efficient.\n",
    "One of those more specialized data structures is the *set*.\n",
    "\n",
    "A set is essentially an impoverished list.\n",
    "Sets cannot contain an element more than once, and they are unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list can contain multiple elements\n",
    "example_list = [\"the\", \"boy\", \"likes\", \"the\", \"girl\"]\n",
    "# converting the list to a set removes all duplicates\n",
    "example_set = set(example_list)\n",
    "\n",
    "print(example_list)\n",
    "print(example_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list has a specific order\n",
    "example_list = [\"first\", \"second\", \"third\", \"fourth\"]\n",
    "# converting the list to a set destroys the order\n",
    "example_set = set(example_list)\n",
    "\n",
    "print(\"Printing list items\")\n",
    "for item in example_list:\n",
    "    print(item)\n",
    "    \n",
    "print(\"\\nPrinting set items\")\n",
    "for item in example_set:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above may print the items in the correct order in both cases, but there is no guarantee that this always happens.\n",
    "In principle, Python can pull the items from the set in any order it wants.\n",
    "\n",
    "Alright, so sets are a variant of lists that misses two useful properties, order and the ability to contain multiple tokens of the same type.\n",
    "Why would anybody want such an impoverished data structure?\n",
    "\n",
    "Well, sometimes the removal of duplicate entries is a boon rather than a shortcoming.\n",
    "Suppose we want to write a function that tells us for any two strings whether they contain the same characters.\n",
    "This is very easy with sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def char_equivalent(string1, string2):\n",
    "    # convert strings to sets of characters\n",
    "    string1 = set(string1)\n",
    "    string2 = set(string2)\n",
    "    if string1 == string2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# let's run some tests:\n",
    "\n",
    "# the comparison is case sensitive\n",
    "print(char_equivalent(\"Tokyo\", \"Kyoto\"))\n",
    "\n",
    "# but order of characters does not matter, as desired\n",
    "print(char_equivalent(\"tokyo\", \"kyoto\"))\n",
    "\n",
    "# and repetition is also fine\n",
    "print(char_equivalent(\"New York\", \"New New York\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other advantage of sets is that they are much faster with the `in` operator.\n",
    "This means that a statement like `if x in y` is computed much faster if `y` is a set rather than a list.\n",
    "For our stop word removal function, for example, we would have been better off using a set of stop words rather than a list.\n",
    "We can verify this by timing how long the code takes to run with a list of stopwords in comparison to a set of stopwords.\n",
    "\n",
    "First we have to define all the custom functions and variables in the familiar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell first to define the necessary variables\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# we first define custom functions for all individual steps\n",
    "\n",
    "def get_file(text):\n",
    "    if text == \"hamlet\":\n",
    "        urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/1524/pg1524.html\", \"hamlet.txt\")\n",
    "    if text == \"faustus\":\n",
    "        urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/811/pg811.txt\", \"faustus.txt\")\n",
    "    if text == \"johncarter\":\n",
    "        urllib.request.urlretrieve(\"http://www.gutenberg.org/cache/epub/62/pg62.txt\", \"johncarter.txt\")\n",
    "        \n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as text:\n",
    "        return text.read()\n",
    "    \n",
    "def delete_before_line(string, line):\n",
    "    return str.split(string, \"\\n\", line)[-1]\n",
    "\n",
    "def delete_after_line(string, line):\n",
    "    return str.join(\"\\n\", str.split(string, \"\\n\")[:line+1])\n",
    "\n",
    "def hamlet_cleaner(text):\n",
    "    # 0. delete unwanted lines\n",
    "    text = delete_after_line(delete_before_line(text, 366), 10928)\n",
    "    # 1. remove all headers, i.e. lines starting with <h1, <h2, <h3, and so on\n",
    "    text = re.sub(r\"<h[0-9].*\", r\"\", text)\n",
    "    # 2. remove speaker information, i.e. lines of the form <p id=\"id012345789\"...<br/>\n",
    "    text = re.sub(r'<p id=\"id[0-9]*\">[^<]*<br/>', r\"\", text)\n",
    "    # 3. remove html tags, i.e. anything of the form <...>\n",
    "    text = re.sub(r\"<[^>]*>\", r\"\", text)\n",
    "    # 4. remove anything after [ or before ] on a line (this takes care of stage descriptions)\n",
    "    text = re.sub(r\"\\[[^\\]\\n]*\", r\"\", text)\n",
    "    text = re.sub(r\"[^\\[\\n]*\\]\", r\"\", text)\n",
    "    return text\n",
    "\n",
    "def faustus_cleaner(text):\n",
    "    # 0. delete unwanted lines\n",
    "    text = delete_after_line(delete_before_line(text, 139), 2854)\n",
    "    # 1. remove stage information\n",
    "    #    (anything after 10 spaces)\n",
    "    text = re.sub(r\"(\\s){10}[^\\n]*\", r\"\", text)\n",
    "    # 2. remove speaker information\n",
    "    #    (any word in upper caps followed by space or dot)\n",
    "    text = re.sub(r\"[A-Z]{2,}[\\s\\.]\", r\"\", text)\n",
    "    # 3. remove anything between square brackets (this takes care of footnote markers)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", r\"\", text)\n",
    "    return text\n",
    "\n",
    "def johncarter_cleaner(text):\n",
    "    # 0. delete unwanted lines\n",
    "    text = delete_after_line(delete_before_line(text, 234), 6940)\n",
    "    # 1. delete CHAPTER I\n",
    "    # (must be done like this because Roman 1 looks like English I)\n",
    "    text = re.sub(\"CHAPTER I\", \"\", text)\n",
    "    # 2. remove any word in upper caps that is longer than 1 character\n",
    "    text = re.sub(r\"[A-Z]{2,}\", r\"\", text)\n",
    "    # 3. remove anything after [ or before ] on a line\n",
    "    text = re.sub(r\"\\[[^\\]\\n]*\", r\"\", text)\n",
    "    text = re.sub(r\"[^\\[\\n]*\\]\", r\"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(string):\n",
    "    return re.findall(r\"\\w+\", string)\n",
    "\n",
    "def count(token_list):\n",
    "    return Counter(token_list)\n",
    "\n",
    "\n",
    "# and now we have two functions that use all the previous functions\n",
    "# to do all the necessary work for us\n",
    "def get_and_clean(text):\n",
    "    get_file(text)\n",
    "    string = read_file(text + \".txt\")\n",
    "    string = str.lower(string)\n",
    "    # file-specific cleaning steps\n",
    "    if text == \"hamlet\":\n",
    "        return hamlet_cleaner(string)\n",
    "    if text == \"faustus\":\n",
    "        return faustus_cleaner(string)\n",
    "    if text == \"johncarter\":\n",
    "        return johncarter_cleaner(string)\n",
    "\n",
    "hamlet_full = tokenize(get_and_clean(\"hamlet\"))\n",
    "\n",
    "# define stop words\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt\", \"stopwords.txt\")\n",
    "stopwords_list = re.findall(r\"[^\\n]+\", read_file(\"stopwords.txt\"))\n",
    "stopwords_set = set(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a list `stopwords_list` and set `stopwords_set`.\n",
    "Now we can test how long the code takes to execute with `stopwords_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_list():\n",
    "    # empty list of words\n",
    "    words = []\n",
    "\n",
    "    # start for-loop\n",
    "    for token in hamlet_full:\n",
    "        if token not in stopwords_list:\n",
    "            # add token to words\n",
    "            list.append(words, token)\n",
    "        \n",
    "# tell Jupyter to time how long it takes to run the function\n",
    "% time test_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution time depends a lot on the hardware you are running this notebook on.\n",
    "On my computer, the code takes around 100 milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_set():\n",
    "    # empty list of words\n",
    "    words = []\n",
    "\n",
    "    # start for-loop\n",
    "    for token in hamlet_full:\n",
    "        if token not in stopwords_set:\n",
    "            # add token to words\n",
    "            list.append(words, token)\n",
    "        \n",
    "# tell Jupyter to time how long it takes to run the function\n",
    "% time test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code with the set, on the other hand, takes about 10 milliseconds.\n",
    "That's a 10-fold speed increase!\n",
    "But okay, saving 90 milliseconds does not seem all that impressive, either way it's only a fraction of a second.\n",
    "But that's only because the list of stopwords isn't all that long.\n",
    "Let's try this test again with a longer list, our dictionary of 500,000 English words.\n",
    "\n",
    "We'll now check the first 10,000 words of *Hamlet* as to whether they occur in the dictionary.\n",
    "This takes quite a while if the dictionary is a list, but is almost instantaneous if the dictionary is a set.\n",
    "In fact, we will run set test over all of *Hamlet* - that's over 30,000 words rather than just the 10,000 used in the list test.\n",
    "And the function using a dictionary set will still finish much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\"\n",
    "urllib.request.urlretrieve(url, \"words.txt\")\n",
    "dict_string = read_file(\"words.txt\")\n",
    "\n",
    "dict_list = re.findall(\"[^\\n]+\", dict_string)\n",
    "dict_set = set(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_list():\n",
    "    # empty list of words\n",
    "    words = []\n",
    "\n",
    "    # start for-loop\n",
    "    for token in hamlet_full[:10000]:\n",
    "        if token not in dict_list:\n",
    "        # add token to words\n",
    "            list.append(words, token)\n",
    "        \n",
    "# tell Jupyter to time how long it takes to run the function\n",
    "% time test_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_set():\n",
    "    # empty list of words\n",
    "    words = []\n",
    "\n",
    "    # start for-loop\n",
    "    for token in hamlet_full:\n",
    "        if token not in dict_set:\n",
    "        # add token to words\n",
    "            list.append(words, token)\n",
    "        \n",
    "# tell Jupyter to time how long it takes to run the function\n",
    "% time test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my computer, `test_set` takes 10 milliseconds to check all of *Hamlet*, whereas `test_list` takes almost a minute just for the first 10,000 words.\n",
    "That is an enormous speed difference, and it is one that is noticeable in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you should not worry too much about efficiency, in particular in this class.\n",
    "But if you are working on a project on your own and you notice that your program is taking quite a bit longer to run than you'd like, take a closer look: maybe there are some lists you want to convert to sets for faster `in` tests?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
