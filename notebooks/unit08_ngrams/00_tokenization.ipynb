{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# From text to lists\n",
    "\n",
    "The previous unit talked a bit about cleaning up input.\n",
    "You saw how you can use custom functions to remove all kinds of clutter from the input.\n",
    "When you clean up input in this way, you are taking a string and modifying it to get a more suitable string that is easier to work with.\n",
    "But sometimes a string is still not good enough, what you actually want is a list.\n",
    "Instead of the string\n",
    "\n",
    "```python\n",
    "\"This string is a string, not a list, as you can see.\"\n",
    "```\n",
    "\n",
    "you might actually want the list\n",
    "\n",
    "```python\n",
    "[\"This\", \"string\", \"is\", \"a\", \"string\", \"not\", \"a\", \"list\", \"as\", \"you\", \"can\", \"see\"]\n",
    "```\n",
    "\n",
    "The process of breaking up a string into a list of words is called **tokenization**, and it is really important in language technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization the easy way\n",
    "\n",
    "It is very easy to build a basic tokenizer for English.\n",
    "In fact, a few lines of Python code suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# define a custom tokenizer function\n",
    "def tokenize(the_string):\n",
    "    token_list = re.findall(r\"\\w+\", the_string)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def ex_print(example):\n",
    "    print(\"The sentence is:\")\n",
    "    print(example)\n",
    "    print(\"Tokenization yields this list:\")\n",
    "    print(tokenize(example))\n",
    "    \n",
    "    \n",
    "ex_print(\"This is an example sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code above to verify that it indeed converts the example sentence to a list of words.\n",
    "Feel free to change the example sentence a bit and observe how the list changes accordingly.\n",
    "\n",
    "All the real work in the code above is done by the custom function `tokenize`.\n",
    "It takes a single argument, which is given the name `the_string`.\n",
    "The function then does two things.\n",
    "First, it uses a rather cryptic command `re.findall(r\"\\w+\", the_string)`.\n",
    "This is the piece that does all the magic of converting a string into a list of words.\n",
    "We'll see in a moment how exactly it does that.\n",
    "The produced list is stored in the variable `token_list`, which is then returned as the output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Rather than storing the list in a variable and then returning the variable, we can directly return the list.\n",
    "Modify the code below along these lines.\n",
    "The result should consist of only two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(the_string):\n",
    "    token_list = re.findall(r\"\\w+\", the_string)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's turn to the mystery of how the string is actually converted into a list.\n",
    "The process is actually much simpler than it looks.\n",
    "You already know Python's `re` package for working with regular expressions.\n",
    "This package provides the `re.sub` function to modify parts of a string.\n",
    "But it also provides the function `re.findall`.\n",
    "This function takes two arguments:\n",
    "\n",
    "1. The first argument of `re.findall` is a regular expression.\n",
    "1. The second argument of `re.findall` is a string.\n",
    "\n",
    "The function then scans the string from left to right, looking for parts that match the regular expression.\n",
    "When a match is found, it is added to the list of matches.\n",
    "At the end, the function returns the list of all found matches.\n",
    "\n",
    "Here is a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# matching digits in a string\n",
    "digits = re.findall(r\"[0-9]\", \"James Madison had 0 sons and fought the war of 1812.\")\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the regular expression matches every character that is a number between 0 and 9.\n",
    "When the `re.findall` function scans through the string from left to right, the first matching symbol is 0.\n",
    "So at this point the list of matches is `['0']`.\n",
    "Then it has to move right for quite a while without much happening, until it finally encounters the year 1812.\n",
    "Each digit of this year is a symbol that matches the regular expression, so each one gets added to the list of matches.\n",
    "The order in the list corresponds exactly to the order of the digits in the string.\n",
    "\n",
    "Now compare this to the list we get with a slightly different regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# matching numbers in a string\n",
    "numbers = re.findall(r\"[0-9]+\", \"James Madison had 0 sons and fought the war of 1812.\")\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a list of digits, we now have a list of numbers.\n",
    "How come?\n",
    "Recall that `+` in a regular expression means *1 or more instances of*.\n",
    "So now `re.findall` doesn't just look at individual symbols in the string.\n",
    "Instead, it looks for the longest parts of the string that are only built from digits.\n",
    "That's `0` and `1812`.\n",
    "\n",
    "The regular expression `r\"[0-9]+\"` is very similar to the one used in our custom function `tokenize`.\n",
    "There, the regex is `r\"\\w+\"`.\n",
    "So apparently we want `re.findall` to match continuous parts of the string that are built up from whatever `\\w` means.\n",
    "But as you already know, `\\w` is a shorthand for *word character*.\n",
    "So the regex `r\"\\w+\"` matches the maximal parts of the string that are built up from word characters - but that's just a round-about way of saying that it matches words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "The code below uses the tokenizer function on a variety of example sentences.\n",
    "Run the cell multiple times and carefully study how the `re.findall` picks out the matches.\n",
    "Based on your observations, what counts as a word character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# define a custom tokenizer function\n",
    "def tokenize(the_string):\n",
    "    token_list = re.findall(r\"\\w+\", the_string)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def ex_print(example):\n",
    "    print(\"The sentence is:\")\n",
    "    print(example)\n",
    "    print(\"Tokenization yields this list:\")\n",
    "    print(tokenize(example))\n",
    "    \n",
    "    \n",
    "sentences = [\"This is the first example sentence.\",\n",
    "             \"Cuz I'm Batman!\",\n",
    "             \"What???!!\",\n",
    "             \"Engage the hyper-drive!\",\n",
    "             \"True music aficionados listen to Röyksopp, Schweißer, and Sígur Rós...\",\n",
    "             \"Bankers only care about $$$\",\n",
    "             \"My phone number is 555-123-4567\",\n",
    "             \"Stalag 17 might be Billy Wilder's best movie!\",\n",
    "             \":-)\",\n",
    "             \"2 + 2 = 4. This much is obvious!\"]\n",
    "\n",
    "ex_print(random.choice(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*put your description of word characters here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "Regular expressions don't just have `\\w` as a special descriptor for word characters.\n",
    "Among others, there is also `\\d` to pick out digits.\n",
    "Based on the code above for matching digits and numbers, write two custom functions `digit_match` and `number_match`.\n",
    "Test the functions on the example sentence *James Madison had 0 sons and fought the war of 1812*.\n",
    "Whereas `digit_match` should return `['0', '1', '8', '1', '2']`, `number_match` should return `['0', '1812']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def digit_match(the_string):\n",
    "    # complete this function\n",
    "    \n",
    "\n",
    "def number_match(the_string):\n",
    "    # complete this function\n",
    "    \n",
    "    \n",
    "# put your tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So whenever you need to tokenize something, you can follow this simple recipe:\n",
    "\n",
    "```python\n",
    "re.findall(r\"\\w+\", string_to_be_tokenized)\n",
    "```\n",
    "\n",
    "Pretty amazing if you think about it.\n",
    "A single line of code for what might seem like a fairly difficult task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of the simple approach\n",
    "\n",
    "You have probably noticed by now that this approach results in lists that use a somewhat odd notion of word.\n",
    "For instance, the word *non-descriptive* would be split into *non* and *descriptive*.\n",
    "A phone number like *555-1234-5678* will be split into *555*, *1234*, and *5678*, when it is arguably a single word.\n",
    "One solution would be to treat as a word any continuous sequence of characters that does not contain any whitespace.\n",
    "A whitespace character is a space or a tabulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "In regular expressions, `\\s` can be used to match whitespace characters, whereas `\\S` matches anything that is not whitespace.\n",
    "Adapt the code below so that instead of maximal sequences of word characters, it matches maximal sequences of non-whitespace characters.\n",
    "How does this affect tokenization of the example sentences?\n",
    "List at least one specific improvement, and at least one specific regression (that is to say, something this solution does worse than the previous tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# define a custom tokenizer function\n",
    "def tokenize(the_string):\n",
    "    token_list = re.findall(r\"\\w+\", the_string)\n",
    "    return token_list\n",
    "\n",
    "sentences = [\"This is the first example sentence.\",\n",
    "             \"Cuz I'm Batman!\",\n",
    "             \"What???!!\",\n",
    "             \"Engage the hyper-drive!\",\n",
    "             \"True music aficionados listen to Röyksopp, Schweißer, and Sígur Rós...\",\n",
    "             \"Bankers only care about $$$\",\n",
    "             \"My phone number is 555-123-4567\",\n",
    "             \"Stalag 17 might be Billy Wilder's best movie!\",\n",
    "             \":-)\"]\n",
    "\n",
    "example = random.choice(sentences)\n",
    "print(\"The sentence is:\")\n",
    "print(example)\n",
    "print(\"Tokenization yields this list:\")\n",
    "print(tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*put your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "This continues the previous exercise.\n",
    "Modify your code so that each punctuation symbol is treated as a word.\n",
    "So `\"Sue slept.\"` would be tokenized as `[\"Sue\", \"slept\", \".\"]`,\n",
    "`\"Sue, stop!\"` as `[\"Sue\", \",\", \"stop\", \"!\"]`,\n",
    "and `\"Sue and Bill...\"` as `[\"Sue\", \"and\", \"Bill\", \".\", \".\", \".\"]`.\n",
    "\n",
    "*Hints:*\n",
    "If you're stuck with the exercise, highlight the text below to read some tips.\n",
    "\n",
    "<span style=\"color:#000000;background-color:#000000;\">\n",
    "Use re.sub to insert whitespace before punctuation symbols.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your modified code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**\n",
    "You now know that\n",
    "\n",
    "- `\\w` matches word characters\n",
    "- `\\d` matches digits\n",
    "- `\\s` matches whitespace (space & tabs)\n",
    "- `\\S` matches anything that is not matched by `\\s`.\n",
    "\n",
    "So what might `\\W` and `\\D` match?\n",
    "Experiment with `re.findall` in the code cell below to verify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*put your description of \\W and \\D here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a high-quality tokenizer is actually a very challenging task.\n",
    "All kinds of edge cases must be taken into account.\n",
    "For example, how should one tokenize any of the following:\n",
    "\n",
    "- *$20*\n",
    "- *R2-D2*\n",
    "- *www.somedomain.com/query?=5328_iawb;id=293*\n",
    "\n",
    "The answer is not straight-forward and depends on why exactly you want to tokenize strings to begin with.\n",
    "We will see concrete applications in the remainder of this unit.\n",
    "For this application, the tokenizer doesn't have to be top-notch - the regex `r\"\\w+\"` will do the job just fine.\n",
    "But even for such simple applications, tokenization can be very hard depending on the language.\n",
    "English makes our life easy because of the convention that words are separated by spaces.\n",
    "\n",
    "```\n",
    "WedonotwriteEnglishsentenceslikethiswithoutanyspacebetweenwords.\n",
    "```\n",
    "\n",
    "But some languages do not follow this convention, for instance Chinese.\n",
    "Writing a tokenizer for Chinese is a much harder task that requires a good understanding of the language.\n",
    "One short regular expression won't cut it for Chinese.\n",
    "This is a good example of how a piece of language technology might be straight-forward for language X, but really hard for language Y.\n",
    "This happens quite often.\n",
    "In an ideal world, the engineers working on language technology would have a solid background in linguistics that makes them aware of how much languages can vary.\n",
    "Unfortunately, we do not live in this ideal world; many pieces of language technology do not work well for certain languages because the programmers made assumptions that simply do not hold for these languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bullet point summary\n",
    "\n",
    "- Tokenization is the process of converting a sentence or text from a string to a list.\n",
    "- A basic tokenization recipe:\n",
    "\n",
    "```python\n",
    "re.findall(r\"\\w+\", string_to_be_tokenized)\n",
    "```\n",
    "\n",
    "- `re.findall` takes a regular expression R and a string S and returns a list of all matches for R in S.\n",
    "- Regular expressions provides shorthands for matching specific classes:\n",
    "    - `\\w` matches word characters (A-Z, a-z, 0-9),\n",
    "    - `\\d` matches digits (0-9),\n",
    "    - `\\s` matches whitespace,\n",
    "    - `\\W`, `\\D`, and `\\S` match whatever is not matched by `\\w`, `\\d`, and `\\s`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
